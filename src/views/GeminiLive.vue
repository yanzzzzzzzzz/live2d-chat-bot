<template>
  <div class="gemini-live-container">
    <div class="controls-panel">
      <button 
        class="connect-btn" 
        :class="{ connected: isConnected, connecting: isConnecting }"
        @click="toggleConnection"
        :disabled="isConnecting"
      >
        {{ connectionButtonText }}
      </button>
      
      <div class="api-key-input" v-if="!isConnected">
        <input 
          v-model="apiKey" 
          type="password" 
          placeholder="è«‹è¼¸å…¥ Gemini API Key"
          class="api-input"
          @input="clearError"
        />
        <small class="api-hint">
          è«‹ç¢ºä¿æ‚¨çš„API Keyæœ‰æ•ˆä¸¦ä¸”å·²å•Ÿç”¨Gemini LiveåŠŸèƒ½
        </small>
      </div>
      
      <div class="voice-controls" v-if="isConnected">
        <div class="voice-status">
          <div class="status-text">
            {{ isListening ? 'ğŸ¤ è½å–ä¸­ - è«‹èªªè©±' : 'ğŸ”‡ æœªåœ¨è½å–' }}
          </div>
          <div class="volume-indicator">
            <AudioPulse :active="isListening" :volume="inputVolume" />
            <span class="volume-text">éŸ³é‡: {{ Math.round(inputVolume * 100) }}%</span>
          </div>
        </div>
        <button @click="toggleListening" class="voice-btn">
          {{ isListening ? 'åœæ­¢è½å–' : 'é–‹å§‹è½å–' }}
        </button>
      </div>
      
      <div class="status-info">
        <p>é€£ç·šç‹€æ…‹: {{ connectionStatus }}</p>
        <p v-if="lastError" class="error">éŒ¯èª¤: {{ lastError }}</p>
        <details v-if="lastError" class="debug-info">
          <summary>èª¿è©¦ä¿¡æ¯</summary>
          <p>API Key é•·åº¦: {{ apiKey.length }}</p>
          <p>API Key é–‹é ­: {{ apiKey.substring(0, 10) }}...</p>
          <p>è«‹ç¢ºä¿API Keyæ­£ç¢ºä¸”æœ‰Gemini Liveæ¬Šé™</p>
        </details>
      </div>
    </div>

    <ModelCanvas ref="canvasRef" :initialScale="modelScale" />
    <AudioControls @play="onPlay" @stop="onStop" />
    <ScaleControls :scale="modelScale" :presets="presets" @zoom-in="onZoomIn" @zoom-out="onZoomOut" @set-scale="onSetScale" />
    <OffsetControls :x="modelOffsetX" :y="modelOffsetY" :minX="offsetMinX" :maxX="offsetMaxX" :minY="offsetMinY" :maxY="offsetMaxY" @update:x="onUpdateX" @update:y="onUpdateY" @reset-x="onResetX" @reset-y="onResetY" @reset-all="onResetAll" />
  </div>
</template>

<script setup lang="ts">
import { ref, computed, onMounted, onUnmounted } from 'vue'
import ModelCanvas from '@/components/ModelCanvas.vue'
import AudioControls from '@/components/AudioControls.vue'
import AudioPulse from '@/components/AudioPulse.vue'
import ScaleControls from '@/components/ScaleControls.vue'
import OffsetControls from '@/components/OffsetControls.vue'
import { useLiveAPI } from '@/composables/useLiveAPI'
import { AudioRecorder } from '@/lib/audio-recorder'

const modelScale = ref(0.5)
const presets = [0.05, 0.1, 0.2, 0.5, 1]

const modelOffsetX = ref(0)
const modelOffsetY = ref(0)

const offsetMinX = ref(-400)
const offsetMaxX = ref(400)
const offsetMinY = ref(-800)
const offsetMaxY = ref(800)

const canvasRef = ref<InstanceType<typeof ModelCanvas> | null>(null)

// Gemini Live APIç›¸é—œç‹€æ…‹
const apiKey = ref('')
const isConnecting = ref(false)
const isConnected = ref(false)
const lastError = ref('')
const isListening = ref(false)
let mediaRecorder: MediaRecorder | null = null
let audioRecorder: AudioRecorder | null = null

// éŸ³é‡ç‹€æ…‹
const inputVolume = ref(0)

// éŸ³é »ç·©å­˜ç›¸é—œè®Šé‡
const audioChunks = ref<ArrayBuffer[]>([])
const currentAudioUrl = ref<string | null>(null)
let audioCollecting = false

// ä½¿ç”¨Live API
let liveAPI: ReturnType<typeof useLiveAPI> | null = null

const connectionStatus = computed(() => {
  if (isConnecting.value) return 'é€£æ¥ä¸­...'
  if (isConnected.value) return 'å·²é€£æ¥'
  return 'æœªé€£æ¥'
})

const connectionButtonText = computed(() => {
  if (isConnecting.value) return 'é€£æ¥ä¸­...'
  if (isConnected.value) return 'æ–·é–‹é€£æ¥'
  return 'é€£ç·šåˆ°Gemini Live'
})

// åˆå§‹åŒ–Live API
const initLiveAPI = () => {
  if (!apiKey.value.trim()) {
    lastError.value = 'è«‹è¼¸å…¥API Key'
    return
  }

  try {
    liveAPI = useLiveAPI({ apiKey: apiKey.value })
    
    // è¨­ç½®é…ç½®
    liveAPI.setConfig({
      generationConfig: {
        responseModalities: ['AUDIO'] as any,
        speechConfig: {
          voiceConfig: { 
            prebuiltVoiceConfig: { 
              voiceName: 'Charon'
            } 
          }
        }
      },
      systemInstruction: {
        parts: [{
          text: 'You are a friendly AI assistant. Please respond in English with short, natural responses.'
        }]
      }
    })

    // ç›£è½é€£æ¥ç‹€æ…‹è®ŠåŒ–
    liveAPI.client.on('open', () => {
      console.log('Connection opened')
      isConnected.value = true
      isConnecting.value = false
      lastError.value = ''
    })

    liveAPI.client.on('close', () => {
      console.log('Connection closed')
      isConnected.value = false
      isConnecting.value = false
      // é€£ç·šæ–·é–‹æ™‚åœæ­¢è½å–
      stopListening()
    })

    liveAPI.client.on('error', (error) => {
      console.error('Live API error:', error)
      lastError.value = error.message || 'é€£æ¥éŒ¯èª¤'
      isConnecting.value = false
      isConnected.value = false
    })

    // ç›£è½éŸ³é »æ•¸æ“šä¸¦æ”¶é›†åˆ°ç·©å­˜ä¸­
    liveAPI.client.on('audio', (audioData: ArrayBuffer) => {
      console.log('ğŸµ Received audio data:', audioData.byteLength, 'bytes')
      
      // é–‹å§‹æ”¶é›†éŸ³é »æ•¸æ“š
      if (!audioCollecting) {
        console.log('ğŸ¬ Starting audio collection...')
        audioChunks.value = []
        audioCollecting = true
      }
      
      // å°‡éŸ³é »æ•¸æ“šæ·»åŠ åˆ°ç·©å­˜ä¸­
      audioChunks.value.push(audioData.slice()) // å‰µå»ºå‰¯æœ¬
      console.log('ğŸ“¦ Collected audio chunk, total chunks:', audioChunks.value.length)
    })

    liveAPI.client.on('turncomplete', () => {
      console.log('ğŸ Turn complete - conversation round ended')
      
      // å¦‚æœæ­£åœ¨æ”¶é›†éŸ³é »æ•¸æ“šï¼Œç¾åœ¨è™•ç†å®ƒå€‘
      if (audioCollecting && audioChunks.value.length > 0) {
        console.log('ğŸ”„ Processing collected audio data...')
        processCollectedAudio()
        audioCollecting = false
      }
    })

    liveAPI.client.on('interrupted', () => {
      console.log('âš ï¸ Interrupted')
      
      // å¦‚æœæ­£åœ¨æ”¶é›†éŸ³é »æ•¸æ“šï¼Œç¾åœ¨è™•ç†å®ƒå€‘
      if (audioCollecting && audioChunks.value.length > 0) {
        console.log('ğŸ”„ Processing collected audio data (interrupted)...')
        processCollectedAudio()
      }
      audioCollecting = false
    })

    liveAPI.client.on('content', (data) => {
      console.log('ğŸ“ Received content:', data)
    })

    liveAPI.client.on('setupcomplete', () => {
      console.log('âœ… Setup completed - ready for conversation')
      // é€£ç·šè¨­å®šå®Œæˆå¾Œï¼Œè‡ªå‹•é–‹å§‹éŸ³é »éŒ„è£½
      startListening()
    })

    lastError.value = ''
  } catch (error: any) {
    console.error('Failed to initialize Live API:', error)
    lastError.value = 'åˆå§‹åŒ–å¤±æ•—: ' + (error.message || error)
    isConnecting.value = false
    isConnected.value = false
  }
}

// é€£æ¥/æ–·é–‹é€£æ¥
const toggleConnection = async () => {
  if (isConnected.value) {
    await disconnect()
  } else {
    await connect()
  }
}

const connect = async () => {
  if (!liveAPI) {
    initLiveAPI()
  }

  if (!liveAPI) {
    return
  }

  try {
    isConnecting.value = true
    isConnected.value = false
    lastError.value = ''
    
    console.log('Attempting to connect to Gemini Live API...')
    await liveAPI.connect()
    console.log('Connect call completed')
  } catch (error: any) {
    console.error('Connection failed:', error)
    lastError.value = 'é€£æ¥å¤±æ•—: ' + (error.message || error)
    isConnecting.value = false
    isConnected.value = false
  }
}

const disconnect = async () => {
  if (liveAPI) {
    await liveAPI.disconnect()
    stopListening()
  }
  isConnected.value = false
  isConnecting.value = false
}

// æ¸…é™¤éŒ¯èª¤ä¿¡æ¯
const clearError = () => {
  lastError.value = ''
}

// è™•ç†æ”¶é›†çš„éŸ³é »æ•¸æ“šä¸¦è½‰æ›ç‚ºå¯æ’­æ”¾çš„éŸ³é »æ–‡ä»¶
const processCollectedAudio = async () => {
  if (audioChunks.value.length === 0) {
    console.log('âš ï¸ No audio chunks to process')
    return
  }

  console.log('ğŸ”§ Processing', audioChunks.value.length, 'audio chunks...')

  try {
    // è¨ˆç®—ç¸½å­—ç¯€æ•¸
    const totalBytes = audioChunks.value.reduce((sum, chunk) => sum + chunk.byteLength, 0)
    console.log('ğŸ“Š Total audio data:', totalBytes, 'bytes')

    // åˆä½µæ‰€æœ‰éŸ³é »å¡Š
    const combinedBuffer = new Uint8Array(totalBytes)
    let offset = 0
    
    for (const chunk of audioChunks.value) {
      combinedBuffer.set(new Uint8Array(chunk), offset)
      offset += chunk.byteLength
    }

    // å°‡ PCM16 æ•¸æ“šè½‰æ›ç‚º WAV æ ¼å¼
    const wavBuffer = createWavFile(combinedBuffer, 24000, 1) // 24kHz, mono
    const audioBlob = new Blob([wavBuffer], { type: 'audio/wav' })
    
    // æ¸…ç†ä¹‹å‰çš„éŸ³é » URL
    if (currentAudioUrl.value) {
      URL.revokeObjectURL(currentAudioUrl.value)
    }
    
    // å‰µå»ºæ–°çš„éŸ³é » URL
    currentAudioUrl.value = URL.createObjectURL(audioBlob)
    console.log('âœ… Audio file created:', currentAudioUrl.value)
    
    // ä½¿ç”¨ Live2D çš„æ’­æ”¾åŠŸèƒ½æ’­æ”¾éŸ³é »ä¸¦åŒæ­¥å˜´å‹
    if (canvasRef.value && currentAudioUrl.value) {
      console.log('ğŸ­ Playing audio with Live2D lip sync...')
      canvasRef.value.playVoice?.(currentAudioUrl.value)
    }
    
    // æ¸…ç©ºç·©å­˜
    audioChunks.value = []
    
  } catch (error) {
    console.error('âŒ Error processing audio:', error)
  }
}

// å‰µå»º WAV æ–‡ä»¶çš„è¼”åŠ©å‡½æ•¸
const createWavFile = (pcmData: Uint8Array, sampleRate: number, channels: number): ArrayBuffer => {
  const length = pcmData.length
  const buffer = new ArrayBuffer(44 + length)
  const view = new DataView(buffer)
  
  // WAV æª”é ­
  const writeString = (offset: number, string: string) => {
    for (let i = 0; i < string.length; i++) {
      view.setUint8(offset + i, string.charCodeAt(i))
    }
  }
  
  writeString(0, 'RIFF')
  view.setUint32(4, 36 + length, true)
  writeString(8, 'WAVE')
  writeString(12, 'fmt ')
  view.setUint32(16, 16, true) // PCM format
  view.setUint16(20, 1, true) // PCM
  view.setUint16(22, channels, true)
  view.setUint32(24, sampleRate, true)
  view.setUint32(28, sampleRate * channels * 2, true)
  view.setUint16(32, channels * 2, true)
  view.setUint16(34, 16, true) // 16-bit
  writeString(36, 'data')
  view.setUint32(40, length, true)
  
  // PCM æ•¸æ“š
  new Uint8Array(buffer, 44).set(pcmData)
  
  return buffer
}

// éŸ³é »éŒ„è£½ç›¸é—œ
const startListening = async () => {
  if (!isConnected.value) return

  try {
    // å•Ÿå‹•éŸ³é‡æª¢æ¸¬
    if (!audioRecorder) {
      audioRecorder = new AudioRecorder()
      audioRecorder.on('volume', (volume: number) => {
        inputVolume.value = volume
      })
    }
    
    await audioRecorder.start()
    console.log('ğŸ¤ Audio recorder started for volume detection')

    // ç²å–éŸ³é »æµ
    const stream = await navigator.mediaDevices.getUserMedia({ 
      audio: {
        sampleRate: 16000,
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true
      }
    })
    
    // å‰µå»ºéŸ³é »ä¸Šä¸‹æ–‡ç”¨æ–¼PCMè™•ç†
    const audioContext = new AudioContext({ sampleRate: 16000 })
    const source = audioContext.createMediaStreamSource(stream)
    
    // å‰µå»ºè…³æœ¬è™•ç†å™¨ç¯€é»
    const processor = audioContext.createScriptProcessor(4096, 1, 1)
    
    processor.onaudioprocess = (event) => {
      if (!liveAPI || !isListening.value) return
      
      const inputBuffer = event.inputBuffer
      const inputData = inputBuffer.getChannelData(0)
      
      // è½‰æ› Float32Array ç‚º Int16Array (PCM16)
      const pcm16 = new Int16Array(inputData.length)
      for (let i = 0; i < inputData.length; i++) {
        const sample = Math.max(-1, Math.min(1, inputData[i]))
        pcm16[i] = sample < 0 ? sample * 0x8000 : sample * 0x7FFF
      }
      
      // è½‰æ›ç‚º base64
      const buffer = pcm16.buffer
      const bytes = new Uint8Array(buffer)
      let binary = ''
      for (let i = 0; i < bytes.length; i++) {
        binary += String.fromCharCode(bytes[i])
      }
      const base64 = btoa(binary)
      
      // ç™¼é€ PCM æ•¸æ“šçµ¦ Gemini Live API
      liveAPI.client.sendRealtimeInput([{
        mimeType: 'audio/pcm;rate=16000',
        data: base64
      }])
    }
    
    // é€£æ¥éŸ³é »ç¯€é»
    source.connect(processor)
    processor.connect(audioContext.destination)
    
    isListening.value = true
    console.log('ğŸ¤ Started listening with PCM16 format')
    
    // ä¿å­˜å¼•ç”¨ä»¥ä¾¿æ¸…ç†
    mediaRecorder = {
      stop: () => {
        processor.disconnect()
        source.disconnect()
        audioContext.close()
        stream.getTracks().forEach(track => track.stop())
      },
      stream: stream
    } as any
    
  } catch (error) {
    console.error('Failed to start listening:', error)
    lastError.value = 'ç„¡æ³•é–‹å§‹éŒ„éŸ³: ' + (error as Error).message
  }
}

const stopListening = () => {
  if (mediaRecorder && isListening.value) {
    mediaRecorder.stop()
    mediaRecorder = null
    isListening.value = false
  }
  
  // åœæ­¢éŸ³é‡æª¢æ¸¬
  if (audioRecorder) {
    audioRecorder.stop()
    audioRecorder = null
    inputVolume.value = 0
  }
}

const toggleListening = () => {
  if (isListening.value) {
    stopListening()
  } else {
    startListening()
  }
}

// Live2Dæ§åˆ¶å‡½æ•¸
function onZoomIn() {
  modelScale.value = Math.min(modelScale.value + 0.05, 2)
  canvasRef.value?.setScale?.(modelScale.value)
}
function onZoomOut() {
  modelScale.value = Math.max(modelScale.value - 0.05, 0.01)
  canvasRef.value?.setScale?.(modelScale.value)
}
function onSetScale(s: number) {
  modelScale.value = s
  canvasRef.value?.setScale?.(s)
}

function onPlay(audioUrl: string) {
  if (!audioUrl) return
  canvasRef.value?.playVoice?.(audioUrl)
}
function onStop() {
  canvasRef.value?.stopSpeaking?.()
}

function onUpdateX(v: number) {
  modelOffsetX.value = v
  canvasRef.value?.applyOffset?.(modelOffsetX.value, modelOffsetY.value, offsetMinX.value, offsetMaxX.value, offsetMinY.value, offsetMaxY.value)
}
function onUpdateY(v: number) {
  modelOffsetY.value = v
  canvasRef.value?.applyOffset?.(modelOffsetX.value, modelOffsetY.value, offsetMinX.value, offsetMaxX.value, offsetMinY.value, offsetMaxY.value)
}

function onResetX() { modelOffsetX.value = 0; onUpdateX(0) }
function onResetY() { modelOffsetY.value = 0; onUpdateY(0) }
function onResetAll() { modelOffsetX.value = 0; modelOffsetY.value = 0; onUpdateX(0); onUpdateY(0) }

// ç”Ÿå‘½é€±æœŸ
onMounted(() => {
  // å¯ä»¥åœ¨é€™è£¡åŠ è¼‰å­˜å„²çš„API Key
})

onUnmounted(() => {
  disconnect()
  stopListening()
})
</script>

<style scoped>
.gemini-live-container {
  position: relative;
  width: 100%;
  height: 100vh;
}

.controls-panel {
  position: absolute;
  top: 20px;
  left: 20px;
  z-index: 100;
  background: rgba(0, 0, 0, 0.8);
  border-radius: 10px;
  padding: 20px;
  color: #fff;
  min-width: 300px;
}

.connect-btn {
  display: inline-block;
  padding: 12px 20px;
  background: rgba(255, 255, 255, 0.1);
  color: #fff;
  border: 1px solid rgba(255, 255, 255, 0.2);
  border-radius: 8px;
  cursor: pointer;
  transition: all 0.3s ease;
  font-size: 14px;
  font-weight: 500;
}

.connect-btn:hover {
  background: rgba(255, 255, 255, 0.2);
  border-color: rgba(255, 255, 255, 0.4);
}

.connect-btn.connected {
  background: rgba(34, 197, 94, 0.2);
  border-color: rgba(34, 197, 94, 0.4);
  color: #22c55e;
}

.connect-btn.connecting {
  background: rgba(251, 191, 36, 0.2);
  border-color: rgba(251, 191, 36, 0.4);
  color: #fbbf24;
}

.connect-btn:disabled {
  opacity: 0.6;
  cursor: not-allowed;
}

.api-key-input {
  margin-top: 15px;
}

.api-input {
  width: 100%;
  padding: 10px;
  border: 1px solid rgba(255, 255, 255, 0.2);
  border-radius: 6px;
  background: rgba(255, 255, 255, 0.1);
  color: #fff;
  font-size: 14px;
}

.api-input::placeholder {
  color: rgba(255, 255, 255, 0.6);
}

.api-hint {
  display: block;
  margin-top: 5px;
  color: rgba(255, 255, 255, 0.5);
  font-size: 11px;
  line-height: 1.3;
}

.voice-controls {
  margin-top: 15px;
}

.voice-status {
  margin-bottom: 10px;
  padding: 8px 12px;
  background: rgba(255, 255, 255, 0.05);
  border-radius: 6px;
  font-size: 12px;
  color: rgba(255, 255, 255, 0.8);
}

.status-text {
  text-align: center;
  margin-bottom: 8px;
}

.volume-indicator {
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 8px;
}

.volume-text {
  font-size: 10px;
  color: rgba(255, 255, 255, 0.6);
  min-width: 60px;
}

.voice-btn {
  width: 100%;
  padding: 8px 16px;
  background: rgba(59, 130, 246, 0.2);
  border: 1px solid rgba(59, 130, 246, 0.4);
  border-radius: 6px;
  color: #3b82f6;
  cursor: pointer;
  transition: all 0.3s ease;
  font-size: 12px;
}

.voice-btn:hover {
  background: rgba(59, 130, 246, 0.3);
}

.voice-btn:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.status-info {
  margin-top: 15px;
  font-size: 12px;
}

.status-info p {
  margin: 5px 0;
  color: rgba(255, 255, 255, 0.8);
}

.error {
  color: #ef4444 !important;
}

.debug-info {
  margin-top: 10px;
  padding: 8px;
  background: rgba(255, 255, 255, 0.05);
  border-radius: 4px;
  font-size: 11px;
}

.debug-info summary {
  cursor: pointer;
  color: rgba(255, 255, 255, 0.7);
  margin-bottom: 5px;
}

.debug-info p {
  margin: 3px 0;
  color: rgba(255, 255, 255, 0.6);
}
</style>
